{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, mnist, CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTargetTransform:\n",
    "    def __init__(self, num_classes=10):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, target):\n",
    "        new_target = torch.zeros(self.num_classes, dtype=torch.float, device=device)\n",
    "        new_target[target] = 1\n",
    "        return new_target\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Lambda(lambda x: x.float().to(device))\n",
    "])\n",
    "\n",
    "# data_loader = DataLoader(dataset, batch_size=800, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# dataset = mnist.FashionMNIST(\"data\", download=True, train=True, transform=transform, target_transform=CustomTargetTransform())\n",
    "# dataset_target = mnist.FashionMNIST(\"data\", download=True, train=False, transform=transforms.PILToTensor())\n",
    "dataset = CIFAR10(\"data\", download=True, train=True, transform=transform, target_transform=CustomTargetTransform())\n",
    "dataset_target = CIFAR10(\"data\", download=True, train=False, transform=transforms.PILToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 3, 32, 32]), torch.Size([10000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data = torch.tensor(dataset_target.data).swapaxes(3, 1).float().to(device)\n",
    "# target_data = torch.tensor(dataset_target.data).unsqueeze(1).float().to(device)\n",
    "target_labels = torch.tensor(dataset_target.targets).float().to(device)\n",
    "target_data.shape, target_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, batch_norm1, batch_norm2, inter_channels, kernel_size1, kernel_size2, stride1, stride2, padding1=1, padding2=1):\n",
    "        super().__init__()\n",
    "        block = [\n",
    "            nn.Conv2d(in_channels, inter_channels, kernel_size1, stride1, padding1),\n",
    "        ]\n",
    "        if batch_norm1:\n",
    "            block.append(nn.BatchNorm2d(inter_channels))\n",
    "        block.append(nn.ReLU())\n",
    "        block.append(nn.Conv2d(inter_channels, in_channels, kernel_size2, stride2, padding2))\n",
    "        if batch_norm2:\n",
    "            block.append(nn.BatchNorm2d(in_channels))\n",
    "        self.block = nn.Sequential(\n",
    "            *block\n",
    "        )\n",
    "    \n",
    "    def get_out_size(self, in_size):\n",
    "        for layer in (self.block[0], self.block[-1] if type(self.block[-1]) == nn.Conv2d else self.block[-2]):\n",
    "            in_size = (in_size - layer.kernel_size[0] + 2 * layer.padding[0]) // layer.stride[0] + 1\n",
    "        return in_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + self.block(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CIFAR10'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_NAME = str(dataset.__class__.__name__).split(\".\")[-1]\n",
    "TRAIN_ID = 0\n",
    "CHECKPOINT_DIR = Path(\"check\")\n",
    "DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(\n",
    "        img_size,\n",
    "        config: dict[str, int],\n",
    "        *layers,\n",
    "        linears: list[int],\n",
    "        epoch_count=10,\n",
    "        softmax=True,\n",
    "        in_channels=1,\n",
    "    ) -> tuple[nn.Sequential, nn.CrossEntropyLoss, torch.optim.SGD, SummaryWriter]:\n",
    "    global TRAIN_ID\n",
    "    torch.manual_seed(0)\n",
    "    blocks = []\n",
    "    outs = img_size\n",
    "    for layer in layers:\n",
    "        blocks.append(\n",
    "            layer\n",
    "        )\n",
    "        tpe = type(layer)\n",
    "        if tpe == ResBlock:\n",
    "            outs = layer.get_out_size(outs)\n",
    "        elif tpe == nn.Conv2d:\n",
    "            outs = (outs - layer.kernel_size[0] + 2 * layer.padding[0]) // layer.stride[0] + 1\n",
    "            in_channels = layer.out_channels\n",
    "        elif tpe == nn.MaxPool2d:\n",
    "            outs = (outs - layer.kernel_size) // layer.stride + 1\n",
    "    outs = outs * outs * in_channels\n",
    "    blocks.append(nn.Flatten(1))\n",
    "    for layer in linears:\n",
    "        blocks.append(nn.Linear(outs, layer))\n",
    "        blocks.append(nn.ReLU())\n",
    "        outs = layer\n",
    "    blocks.append(nn.Linear(outs, 10))\n",
    "    if softmax:\n",
    "        blocks.append(nn.Softmax())\n",
    "    model = nn.Sequential(\n",
    "        *blocks,\n",
    "    ).to(device)\n",
    "    er_f = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "    name = f\"{DATASET_NAME}_{TRAIN_ID}\"\n",
    "    TRAIN_ID += 1\n",
    "    print(name)\n",
    "    return model, er_f, optim, name, config, epoch_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, er_f, optim, name, config, epoch_count):\n",
    "    checkpoint = get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"]\n",
    "            model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "            optim.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "        writer = SummaryWriter(comment=name, log_dir=checkpoint_dir)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        writer = SummaryWriter(comment=name)\n",
    "\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=config[\"batch_count\"], shuffle=True)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(start_epoch, epoch_count)):\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for image, target in tqdm_notebook(data_loader, leave=False):\n",
    "            # zero the parameter gradients\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(image)\n",
    "            loss = er_f(outputs, target)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            percent: torch.tensor = ((outputs.max(1).indices == target.max(1).indices).sum() / len(target))\n",
    "            writer.add_scalar(\"loss\", loss, epoch)\n",
    "            writer.add_scalar(\"train_accuracy\", percent, epoch)\n",
    "            predicted = model(target_data)\n",
    "            predicted_labels = predicted.max(1).indices\n",
    "            percent: torch.tensor = ((predicted_labels == target_labels).sum() / len(target_labels))\n",
    "            writer.add_scalar(\"test_accuracy\", percent, epoch)\n",
    "    \n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optim.state_dict(),\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"wb\") as fp:\n",
    "                pickle.dump(checkpoint_data, fp)\n",
    "\n",
    "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "            train.report(\n",
    "                {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "                checkpoint=checkpoint,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
