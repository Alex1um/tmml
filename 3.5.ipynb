{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron via Torch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)  \n",
    "(x_train_raw, y_train_raw),(x_test_raw, y_test_raw) = MnistDataloader(\n",
    "    \"data/mnist/train-images.idx3-ubyte\",\n",
    "    \"data/mnist/train-labels.idx1-ubyte\",\n",
    "    \"data/mnist/t10k-images.idx3-ubyte\",\n",
    "    \"data/mnist/t10k-labels.idx1-ubyte\"\n",
    "    ).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000]), torch.uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.tensor(np.array(x_train_raw).reshape(60000, 28*28))\n",
    "y_train = torch.tensor(np.array(y_train_raw))\n",
    "x_test = torch.tensor(np.array(x_test_raw).reshape(10000, 28*28))\n",
    "y_test = torch.tensor(np.array(y_test_raw))\n",
    "x_train.shape, y_train.shape, x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"84class.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 10),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "er_f = nn.HuberLoss()\n",
    "# er_f = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.1, momentum=.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "for x, y in zip(x_train[:N], y_train[:N]):\n",
    "    target = torch.zeros(10)\n",
    "    target[int(y)] = 1\n",
    "    # y_pred = torch.argmax(y_pred)\n",
    "    optim.zero_grad()\n",
    "    train = Variable(x.float(), requires_grad=True)\n",
    "    target = Variable(target.float(), requires_grad=False)\n",
    "    y_pred = model(train).float()\n",
    "    loss = er_f(y_pred, target)\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.ones(x_train.shape[0], 10) * 0\n",
    "for i, y in zip(range(x_train.shape[0]), y_train):\n",
    "    target[i, int(y)] = 1\n",
    "target = Variable(target.float(), requires_grad=False)\n",
    "train = Variable(x_train.float(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0301, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0301, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0300, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0300, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[317], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(train[current_batch:current_batch \u001b[38;5;241m+\u001b[39m batch])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m er_f(y_pred, target[current_batch:current_batch \u001b[38;5;241m+\u001b[39m batch])\n\u001b[0;32m----> 7\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      9\u001b[0m current_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch = 30000\n",
    "current_batch = 0\n",
    "for epich in range(400):\n",
    "    optim.zero_grad()\n",
    "    y_pred = model(train[current_batch:current_batch + batch]).float()\n",
    "    loss = er_f(y_pred, target[current_batch:current_batch + batch])\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    current_batch += batch\n",
    "    current_batch %= x_train.shape[0]\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4630)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(torch.argmax(model(x_test.float()), 1), y_test).sum() / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = {i: 0 for i in range(10)}\n",
    "for true, predicted in zip(y_test, torch.argmax(model(x_test.float()), 1)):\n",
    "    true = int(true)\n",
    "    errors[true] += true != int(predicted)\n",
    "plt.xticks(range(10))\n",
    "plt.bar(errors.keys(), errors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"45class.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
